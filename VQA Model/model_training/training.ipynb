{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easy-vqa\n",
      "  Downloading easy_vqa-1.0-py3-none-any.whl (3.2 MB)\n",
      "Installing collected packages: easy-vqa\n",
      "Successfully installed easy-vqa-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install easy-vqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easy_vqa import get_train_questions, get_test_questions\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions, train_answers, train_image_ids = get_train_questions()\n",
    "test_questions, test_answers, test_image_ids = get_test_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframes(questions,answers,image_names,type):\n",
    "    \n",
    "    data_info = {\"question\":[],\"answer\":[],\n",
    "                 \"image_path\":[]}\n",
    "    \n",
    "    \n",
    "    for question,answer,image_name in zip(questions,answers,image_names):\n",
    "        data_info['question'].append(question)\n",
    "        data_info['answer'].append(answer)\n",
    "        data_info['image_path'].append(f\"./data/{type}/images/{image_name}.png\")\n",
    "        \n",
    "        \n",
    "    return pd.DataFrame(data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = make_dataframes(train_questions, train_answers, train_image_ids,\"train\")\n",
    "test_df = make_dataframes(test_questions, test_answers, test_image_ids,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is the blue shape?</td>\n",
       "      <td>rectangle</td>\n",
       "      <td>./data/train/images/0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what color is the shape?</td>\n",
       "      <td>blue</td>\n",
       "      <td>./data/train/images/0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does the image contain a rectangle?</td>\n",
       "      <td>yes</td>\n",
       "      <td>./data/train/images/0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is there a triangle in the image?</td>\n",
       "      <td>no</td>\n",
       "      <td>./data/train/images/0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is there a black shape?</td>\n",
       "      <td>no</td>\n",
       "      <td>./data/train/images/0.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question     answer                 image_path\n",
       "0              what is the blue shape?  rectangle  ./data/train/images/0.png\n",
       "1             what color is the shape?       blue  ./data/train/images/0.png\n",
       "2  does the image contain a rectangle?        yes  ./data/train/images/0.png\n",
       "3    is there a triangle in the image?         no  ./data/train/images/0.png\n",
       "4              is there a black shape?         no  ./data/train/images/0.png"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train.csv\")\n",
    "test_df.to_csv(\"test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "### Loading Transformers\n",
    "\n",
    "## Language Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "language_model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vision Model\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "vision_model = AutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameters in language_model.parameters():\n",
    "    parameters.requires_grad = False\n",
    "    \n",
    "for parameters in vision_model.parameters():\n",
    "    parameters.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model move to cuda\n"
     ]
    }
   ],
   "source": [
    "language_model.to(device)\n",
    "vision_model.to(device)\n",
    "print(f\"Model move to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {'black': 6,\n",
    " 'blue': 11,\n",
    " 'brown': 10,\n",
    " 'circle': 0,\n",
    " 'gray': 3,\n",
    " 'green': 1,\n",
    " 'no': 12,\n",
    " 'rectangle': 7,\n",
    " 'red': 2,\n",
    " 'teal': 5,\n",
    " 'triangle': 9,\n",
    " 'yellow': 8,\n",
    " 'yes': 4}\n",
    "\n",
    "num_labels = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader,RandomSampler\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\",index_col=0)\n",
    "test_df = pd.read_csv(\"test.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is the blue shape?</td>\n",
       "      <td>rectangle</td>\n",
       "      <td>./data/train/images/0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what color is the shape?</td>\n",
       "      <td>blue</td>\n",
       "      <td>./data/train/images/0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does the image contain a rectangle?</td>\n",
       "      <td>yes</td>\n",
       "      <td>./data/train/images/0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is there a triangle in the image?</td>\n",
       "      <td>no</td>\n",
       "      <td>./data/train/images/0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is there a black shape?</td>\n",
       "      <td>no</td>\n",
       "      <td>./data/train/images/0.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question     answer                 image_path\n",
       "0              what is the blue shape?  rectangle  ./data/train/images/0.png\n",
       "1             what color is the shape?       blue  ./data/train/images/0.png\n",
       "2  does the image contain a rectangle?        yes  ./data/train/images/0.png\n",
       "3    is there a triangle in the image?         no  ./data/train/images/0.png\n",
       "4              is there a black shape?         no  ./data/train/images/0.png"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    \n",
    "    def __init__(self,df,tokenizer,feature_extractor,lm,vm,label2idx):\n",
    "        \n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.lm = lm\n",
    "        self.vm = vm\n",
    "        self.label2idx= label2idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        df_row = self.data.iloc[idx]\n",
    "        \n",
    "        question = df_row['question']\n",
    "        image_path = df_row['image_path']\n",
    "        \n",
    "        label = self.label2idx.get(df_row['answer'].strip()) ## Encoding\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        text_input = self.tokenizer(question,return_tensors=\"pt\")\n",
    "        image_input = self.feature_extractor(image,return_tensors = \"pt\")\n",
    "        \n",
    "        text_input ={k:v.to(device) for k,v in text_input.items()}\n",
    "        image_input ={k:v.to(device) for k,v in image_input.items()}\n",
    "        \n",
    "        text_tensors = self.lm(**text_input)\n",
    "        image_tensors = self.vm(**image_input)\n",
    "        \n",
    "        text_tensors = text_tensors.pooler_output.view(-1).detach().cpu()\n",
    "        image_tensors = image_tensors.pooler_output.view(-1).detach().cpu()\n",
    "        label = torch.tensor(label,dtype =torch.long)\n",
    "        \n",
    "        \n",
    "        return {\"image_embeddings\":image_tensors,\"text_embeddings\":text_tensors,\"label\":label}\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VQADataset(\n",
    "                train_df,tokenizer,feature_extractor,\n",
    "                language_model,vision_model,label2idx\n",
    ")\n",
    "test_dataset = VQADataset(\n",
    "                test_df,tokenizer,feature_extractor,\n",
    "                language_model,vision_model,label2idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768]), torch.Size([768]), torch.Size([]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['image_embeddings'].size(),out['text_embeddings'].size(),out['label'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_labels):\n",
    "        \n",
    "        super(VQAModel,self).__init__()\n",
    "        \n",
    "        self.num_labels = num_labels\n",
    "        self.fc1 = nn.Linear(768,256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        \n",
    "        self.final_layer = nn.Linear(256,num_labels)\n",
    "        self.parameter = nn.Parameter(torch.Tensor(768,768))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.parameter, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self,image_embeddings,text_embeddings):\n",
    "        \n",
    "        im1 = torch.nn.functional.normalize(image_embeddings)\n",
    "        te = torch.nn.functional.normalize(text_embeddings)\n",
    "        \n",
    "        cross = im1 * te\n",
    "        \n",
    "        weighted = self.relu(torch.mm(cross,self.parameter.t()))\n",
    "        \n",
    "        down = self.bn1(self.fc1(weighted))\n",
    "        \n",
    "        down = self.dropout(down)\n",
    "        \n",
    "        classify = self.final_layer(down)\n",
    "        \n",
    "        # loss = self.criterion(classify.view(-1,self.num_labels),label.view(-1))\n",
    "        \n",
    "        return classify\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQAModel(num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "print(f\"Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_step(model,epochs,training_dataloader,testing_dataloader):\n",
    "    \n",
    "    criterion =  nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=5e-5)\n",
    "    \n",
    "    best_test_accuracy = 0\n",
    "    train_loss = []\n",
    "    testing_loss = []\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        loss_per_batch = []\n",
    "        testing_output = []\n",
    "        true_output = []\n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "        for batch in tqdm(training_dataloader,desc = \"training\"):\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            inputs = {\"image_embeddings\":batch['image_embeddings'],\"text_embeddings\":batch['text_embeddings']}\n",
    "            output = model(**inputs)\n",
    "            \n",
    "            loss = criterion(output.view(-1,num_labels),batch['label'].view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_per_batch.append(loss.detach().cpu().item())\n",
    "            \n",
    "        train_loss.append(np.mean(loss_per_batch))\n",
    "        \n",
    "        \n",
    "        loss_per_batch = []\n",
    "        model.eval()\n",
    "        for batch in tqdm(testing_dataloader,desc = \"testing\"):\n",
    "            \n",
    "            \n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            inputs = {\"image_embeddings\":batch['image_embeddings'],\"text_embeddings\":batch['text_embeddings']}\n",
    "            output = model(**inputs)\n",
    "            \n",
    "            loss = criterion(output.view(-1,num_labels),batch['label'].view(-1))\n",
    "            \n",
    "            loss_per_batch.append(loss.detach().cpu().item())\n",
    "            \n",
    "            pred = output.argmax(-1).detach().cpu().tolist()\n",
    "            testing_output.extend(pred)\n",
    "            true_output.extend(batch['label'].view(-1).detach().cpu().tolist())\n",
    "            \n",
    "        \n",
    "        testing_loss.append(np.mean(loss_per_batch))\n",
    "          \n",
    "        accuracy = accuracy_score(true_output,testing_output)\n",
    "        \n",
    "        print(f\"Epoch {epoch}:- Training Loss : {train_loss[-1]}  Testing Loss: {testing_loss[-1]}  Testing Accuracy: {accuracy}\")\n",
    "        \n",
    "        if accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = accuracy\n",
    "            torch.save(model.state_dict(),\"./best.pt\")\n",
    "            \n",
    "            \n",
    "    \n",
    "    return {\"train_loss\":train_loss,\"test_loss\":testing_loss}\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,batch_size = batch_size,sampler = RandomSampler(train_dataset))\n",
    "test_dataloader = DataLoader(test_dataset,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 1206/1206 [09:02<00:00,  2.22it/s]\n",
      "testing: 100%|██████████| 303/303 [02:18<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:- Training Loss : 1.4915718832122746  Testing Loss: 0.7940468382914074  Testing Accuracy: 0.7384472242323995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 1206/1206 [09:13<00:00,  2.18it/s]\n",
      "testing: 100%|██████████| 303/303 [02:18<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:- Training Loss : 0.739931606682972  Testing Loss: 0.5852659429260606  Testing Accuracy: 0.7909645404734829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 1206/1206 [08:58<00:00,  2.24it/s]\n",
      "testing: 100%|██████████| 303/303 [02:05<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:- Training Loss : 0.5843897338579741  Testing Loss: 0.47143954323856746  Testing Accuracy: 0.8510286364106275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 1206/1206 [09:04<00:00,  2.21it/s]\n",
      "testing: 100%|██████████| 303/303 [02:19<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:- Training Loss : 0.48729665969092256  Testing Loss: 0.4128781789206829  Testing Accuracy: 0.864571487646025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 1206/1206 [09:17<00:00,  2.16it/s]\n",
      "testing: 100%|██████████| 303/303 [02:19<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:- Training Loss : 0.42435762575313224  Testing Loss: 0.37885108260628414  Testing Accuracy: 0.8671560012405666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 1206/1206 [09:25<00:00,  2.13it/s]\n",
      "testing: 100%|██████████| 303/303 [02:22<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:- Training Loss : 0.39401178197876535  Testing Loss: 0.35253101577459783  Testing Accuracy: 0.8714979840793963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 1206/1206 [09:28<00:00,  2.12it/s]\n",
      "testing: 100%|██████████| 303/303 [02:21<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:- Training Loss : 0.3698178409331572  Testing Loss: 0.3461252653067655  Testing Accuracy: 0.8733588338674662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 1206/1206 [09:29<00:00,  2.12it/s]\n",
      "testing: 100%|██████████| 303/303 [02:22<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:- Training Loss : 0.3553437001657229  Testing Loss: 0.3386729648797819  Testing Accuracy: 0.8712912229918329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 1206/1206 [09:12<00:00,  2.18it/s]\n",
      "testing: 100%|██████████| 303/303 [02:04<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:- Training Loss : 0.34332823967113224  Testing Loss: 0.325169440641655  Testing Accuracy: 0.8727385506047762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 1206/1206 [08:18<00:00,  2.42it/s]\n",
      "testing: 100%|██████████| 303/303 [02:05<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:- Training Loss : 0.3336777048151489  Testing Loss: 0.3196681807733605  Testing Accuracy: 0.8781143388814225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history = training_step(model,10,train_dataloader,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccdcc221b52b819e0fb3cc8384c71749690e51b6a92a6e6fb81e81da96c95629"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
